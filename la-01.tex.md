**Theorem**: Let $V_1$ and $V_2$ be subspaces of a vector space $V​$.  
$$
V = V_1 \oplus V_2 \textrm{ (direct sum)}\iff V = V_1 + V_2  \;\rm{and}\; V_1 \cap V_2 = \{0\}
$$
$\Rightarrow​$ 

From the definition of direct sum we have that $V = V_1 + V_2 $. Moreover, $0 \in V_1 \cap V_2$ since $V_1 \cap V_2$ is a subspace.   Now, we take arbitrarily $v \in V_1 \cap V_2$. From the definitions of direct sum and vector spaces we have that $0 = v + (-v)$ where $v \in V_1$ and $-v \in V_2$. But note that $0 \in V_1$ and $0 \in V_2$, e.g, $0 = 0 + 0$ is the unique representation in the direct sum. Accordingly, $v=0$.

$\Leftarrow​$

Suppose that $0 = v_1 + v_2$ where $v_1 \in V_1$ and  $v_2 \in V_2$.  It follows from the definition of vector spaces that $v_1 = -v_2 \in V_2$. Given that, $v_1 \in V_1 \cap V_2$. Moreover, since $V_1 \cap V_2 = {0}$, we have that $v_1 = v_2 = 0$. In other words, $0$ has an unique representation $0=v_1 + v_2$, therefore $V = V_1 \oplus V_2$.



**Proposition**: Let $M$ be a countable set of two or more vectors from a vector space $V$.  
$$
M\;\textrm{is linearly dependent} \iff \exists v_j \in M\; \textrm{s.t.}\; v_j \in span(M - \{v_j\})
$$
$\Rightarrow$ 

Let $M = \{v_1, ..., v_m \} \subset V$ such that $m \ge 2$. Since M is linearly dependent $\exists\; \alpha_1, ..., \alpha_m$ scalars (at least one of them is not zero) such that $\sum_{i=1}^{m} \alpha_i v_j  = 0$. Suppose $\alpha_1$ is the non-zero scalar, then:
$$
v_1 = -\frac{\alpha_2}{\alpha_1}v_2 + ... + \frac{\alpha_m}{\alpha_1}v_m
$$
In other words, $v_1 \in span(M - {v_1})$. 

$\Leftarrow$

Now suppose $\exists v_j \in M, v_j = \beta_1 v_1 + ... + \beta_{j-1} v_{j-1} + \beta_{j+1} v_{j+1} + ... + \beta_m v_m ​$. The previous equation can be rewritten as $- \beta_1 v_1 - ... - \beta_{j-1} v_{j-1} + v_j - \beta_{j+1} v_{j+1} - ... - \beta_m v_m  = 0​$. This shows that $M​$ is linearly dependent ($\alpha_1 = \beta_1, ...,\alpha_{j-1} = - \beta_{j-1}, \alpha_j = 1, \alpha_{j+1} = \beta_{j+1},..., \alpha_{m} = -\beta{m}​$).  



 **Proposition**:  Let  $V$ be a finite dimension vector space and $B = \{v_1,...,v_n\} \subseteq V$.
$$
B\; \textrm{is a base of}\; V \iff \forall v \in V \; \textrm{exists only one representation}\; v = \alpha_1v_1, ..., \alpha_nv_n
$$
where $\alpha_1, ..., \alpha_n​$ are scalars.

$\Rightarrow$ 

We know if $B$ is a base of $V$ then B spans $V$ and B is linearly independent. Therefore, $\exists \alpha_1, ..., \alpha_n$ scalars (at least one nonzero) such that $v = \sum_{i=1}^{n}\alpha_i v_i, v \in Vi$.  We need to show that the previous representation is unique. Suppose  $\beta_1, ..., \beta_n$ (scalars) such that $v = \sum_{i=1}^{n}\beta v_i, v \in V $. From the previous two equations we have that $0 = \sum_{i=1}^{n}(\alpha_1 - \beta_1)v_1$. Since $B$ is a base, we have that that $\alpha_i - \beta_i = 0 \iff \alpha_i = \beta_i​$. In other words, the representation is unique.

$\Leftarrow$

From the proposition (right hand side of the equivalence) we have that $B$ spans $V$. We need to show $B$ is linearly independent. Let $\alpha_1, ..., \alpha_n$ be the scalars that satisfy $0 = \alpha_1 v_1 + ... + \alpha v_n$. Given that $0 = 0v_1 + ... + 0v_n$ and the uniqueness of representation, it follows that $\alpha_i = ...= \alpha_n = 0$. Therefore, $B$ is linearly independent. 

